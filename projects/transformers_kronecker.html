<!doctype html>
<html>

<head>
    <meta charset="utf-8" />
    <link rel="stylesheet" type="text/css" href="../style/style_white.css">
    <title>A Mathematical Description of Transformers via Kronecker Products</title>
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-ER3625DPFD"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-ER3625DPFD');
</script>

<body>

<header>
    <div>
        <nav>
            <li><a class="headlink" href="../index.html">Home</a></li>
            <li><a class="headlink" href="../projects.html">Projects</a></li>
            <li><a class="headlink" href="../about.html">About</a></li>
            <li><a class="headlink" href="../contact.html">Contact</a></li>
            <li><a class="headlink" href="../assets/VasudevMenon_Resume.pdf">Resume</a></li>
        </nav>
    </div>
    <hr width=98%;>
</header>

<main>
    <h1>A Mathematical Description of Transformers via Kronecker Products</h1>
    <p>
        This project provides a <b>mathematical formulation</b> of the <b>Transformer</b> architecture.
        We reformulate the self-attention mechanism using the <b>Kronecker product</b>, revealing how attention can be viewed as a separable but coupled linear operator acting across 
        both the <b>sequence</b> and <b>feature</b> dimensions.
    </p>

    <ul>
        <li><p>Derived a <b>simplified transformer model</b> that isolates attention as a purely linear map without MLPs, biases, or normalization.</p></li>
        <li><p>Formulated attention as \( \mathrm{Attn}(X) = X + W_O \, \mathrm{softmax}((W_Q X)(W_K X)^\top) W_V X \).</p></li>
        <li><p>Proved that attention can be compactly represented as a single Kronecker operator \( (A \otimes (W_O W_V)) X \), 
        where \(A\) governs token interactions and \(W_O W_V\) acts across feature space.</p></li>
        <li><p>Presented <b>algebraic proofs</b> for key Kronecker identities, including the mixed-product property and matrix equivalence \( MXN = (I \otimes M)(N^\top \otimes I)X \).</p></li>
        <li><p>Interpreted this formulation as a step toward <b>mechanistic interpretability</b>, linking matrix structure to functional behavior in transformers.</p></li>
    </ul>

    <p>
        This work bridges <b>deep learning</b> and <b>linear algebra</b>, showing that complex neural computations can often be expressed as elegant tensor products, 
        a step toward understanding the mathematical essence of attention.
    </p>

    <a href="../assets/Transformers_Kronecker.pdf" target="_blank">Read Full Paper (PDF)</a>
</main>

<!-- MathJax -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<footer>
    <hr width=98%;>
    &copy; 2025 Vasudev Menon
</footer>

</body>

</html>
